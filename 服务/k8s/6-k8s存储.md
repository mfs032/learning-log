# 存储分类

## 元数据

configMap：用于保存配置数据(明文)

Secret：用于保存敏感数据(编码)

Downward API：容器在运行时从Kubernetes API服务器获取有关它们自身的信息

## 真实数据

Volume：用于存储临时或者持久性数据

PersistentVolume：申请制的持久化存储





# ConfigMap 

## 模型

![configMap模型](6-k8s%E5%AD%98%E5%82%A8.assets/configMap%E6%A8%A1%E5%9E%8B.png)

```
ConfigMap API提供了像容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者Json二进制等对象
```

```
现在有三台nginx服务器在运行，3台服务器通过LVS方式给客户访问。
现在再来一台服务器，下发配置文件给这3台nginx服务器。
方式一：共享，构建NFS服务器，经配置文件挂载到nginx服务器
方式二：注入，pod从服务器下载配置文件，使用比如cat 1.txt > 2.txt 通过这种方式将原数据覆盖的操作，叫注入

注入的优势：一次注入后，措辞读取不消耗网络和IO
```

## 创建configMap

```shell
第一种：] kubectl create configmap game-config --from-file=1.txt
使用--from-file创建：要求该文件内容是一行一对的 key=value，这种可以在使用时注入pod，变为环境变量
例如：
1.txt
	name=zhangsan
	passwd=123
这样在ConfigMap中文件名1.txt会变成key，文件内容会变成value。注入pod后文件内容的k=v会变成环境变量


第二种：
使用--from-literal直接指定要注入的k=v，
] kubectl create configmap game-config --from-literal=name=dave --from-literal=passwd=123
这种是直接指定要注入的k=v，适合简短的，数量少的k=v


两种方式创建的configMap被注入到pod中后，会被还原成文件的形式，key就是文件名，value就是文件内容
```

```shell
示例
] touch 1.txt
] echo "name=zhangsan" > 1.txt
] echo "passwd=123" >> 1.txt

] kubectl create configmap game-config --from-file=1.txt
] kubectl get cm
] kubectl get cm game-config -o yaml
apiVersion: v1
data:
  1.txt:
    name=zhangsan
    passwd=123
kind: ConfigMap
metadata:
  .....
  
] kubectl create cm game-config --dry-run -o yaml > 2.cm.yaml

] kubectl describe cm game-config
#可以看到文件名和对应的k=v
```

## pod调用configMap

### 第一种

```yaml
pod通过spec.containers.env字段调用configMap

spec:
  containers:
    - name: xxxxx
      image: xxxx
      command: []
      env: #这种是一个一个定义环境变量
        - name: USERNAME #这是pod里要定义的环境变量的key
          valueFrom:
            configMapKeyRef:
              name: literal-config #这是config的name
              key: name #这是literal-config里的k=v中的key
        - name: PASSWORD
          valueFrom:
            configMapKeyRef:
              name: literal-config
              key: passwd
      envFrom: #这种是一次性导入多个多个环境变量
        - configMapRef:
            name: env-config #configMap的name
```

```
上面这种直接引用cm的值，k=v注入到pod里会以环境变量的形式存在，环境变量不会自动更新，像更新需要重启pod。
```

### 第二种

#### 例子

![configMap卷挂载形式](6-k8s%E5%AD%98%E5%82%A8.assets/configMap%E5%8D%B7%E6%8C%82%E8%BD%BD%E5%BD%A2%E5%BC%8F.png)

```shell
上面的spec.containers.volumeMounts是挂载一个卷到pod的一个文件路径下，
该卷的定义在spec.vulumes下，以configMap制成的卷。
这种挂载方式k=v会以文件的形式存在，当cm更新时，对应的pod内的文件也会更新，即支持热更新。


```

挂载后产生的文件

![configMap挂载后的文件](6-k8s%E5%AD%98%E5%82%A8.assets/configMap%E6%8C%82%E8%BD%BD%E5%90%8E%E7%9A%84%E6%96%87%E4%BB%B6.png)

#### 第二种文件热更新的原理

首先是pod内键值文件挂载方式

```
一、键值文件
volume指定挂载到pod内/etc/config
/etc/config/
|--app.conf
   db.host

二、..data隐藏目录
..data目录会出现在pod挂载目录的内部，作为该目录下的一个隐藏子目录。
/etc/
|--..data/

三、节点volume目录
该pod对应节点上kubelet创建的volume对应的子目录/var/lib/kubelet/pods/<Pod UID>/volumes/<类型>/<名称>/
/var/lib/kubelet/pods/123e4567-e89b-12d3-a456-426614174000/volumes/kubernetes.io~configmap/app-config/
├─ ..2024_08_28_10_00_00.123456/  # 版本目录（存储当前配置文件）
│  ├─ app.conf
│  ├─ db.host
├─ ..data -> ..2024_08_28_10_00_00.123456/  # 符号链接，指向当前活跃版本目录
├─ app.conf -> ..data/app.conf  # 符号链接，指向当前版本的 app.conf
└─ db.host -> ..data/db.host    # 符号链接，指向当前版本的 db.host，这个符号链接是给cd到该目录下的用户查看的，

四、映射关系
pod											  节点
/etc/config/		/etc/..data/ --> /var/.../app-config/..data/ --->同目录活跃版本 
|--app.conf	->/etc/..data/app.conf
   db.host	->/etc/..data/db.conf
```

##### 使用软链接的方式

```shell
支持原子性替换
软链接的创建 / 删除 / 修改是原子操作（通过ln -snf命令），可以在不中断应用访问的情况下，瞬间将链接指向新的目标文件（如配置更新时切换版本），避免文件替换过程中出现的内容不完整问题。

适配容器文件系统隔离
容器运行在独立的文件系统命名空间中，无法直接访问节点路径。通过软链接，可在容器内构建与节点存储对应的路径引用，实现跨命名空间的文件共享，同时保持容器内路径的简洁性。
```

###### 原子操作

```
如果使用输入操作更新文件：
1.打开文件，清空文件
2.写入新内容
3.保存文件，退出
这3步在操作系统层面不是原子的，如果在1到2中间有进程在读取文件，会出现进程读到旧内容、空内容或者新内容(完整或不完整)，导致解析错误

创建ln在操作系统层面是原子操作，不需要担心这种情况
```

###### pod进程跨文件系统命名空间访问节点文件

```
被挂载到 Pod 容器内的节点目录（即 Volume 对应的目录），在容器内是完全可见且可访问的—— 你可以通过 ls 等命令直接查看挂载目录，来查看该目录下的所有内容（包括文件、软链接、隐藏目录如 ..data），就像访问容器内的 “本地目录” 一样。
```



##### 使用..data目录的原因

```
保证多文件配置的原子性批量更新
当 ConfigMap/Secret 包含多个键（即多个文件）时，..data作为中间层目录，可通过一次链接切换（更新..data指向的版本目录），实现所有文件的同步更新。避免了逐个切换文件链接导致的 “部分文件已更新、部分未更新” 的中间不一致状态。

简化版本管理与追溯
..data通过指向特定版本目录（如..20240828_100000.123），清晰区分不同时间点的配置版本。旧版本目录会被暂时保留，便于问题排查或回滚操作，解决了多版本配置的存储与切换难题。
```

#### 问题

```
当通过configMap挂载像nginx这种服务的配置文件，虽然配置文件更新，但pod里的nginx服务没有更新。
这是nginx的问题，nginx不支持热更新。

通过更新deploy配置文件中spec.template.metadata.annotations.version/config来指定版本号，触发delpoy的滚动更新。
通过kubectl edit deoloy deploy-name更新，或者
] kubectl patch deployment deploy-name --patch '{"spec":{"template":{"metadata":{"annotations":{"version/config":"124321ncanbaj"}}}}}'打补丁
```

### configMap和Secret不可修改状态

```
如果期望configMap和Secret不可被修改
] kubectl explain configMap
可以看到一个immutable字段置为true，则cm和secret不可变
...
kind: ConfigMap
immutable: true
metadata:
....

该操作不可逆，置为true，则该cm不可修改，只能删除后重新创建
```



# Secret

## Secret默认类型Opaque

### 例子

![Secret-Opaque](6-k8s%E5%AD%98%E5%82%A8.assets/Secret-Opaque.png)

```
Opaque就是适用于存储任意的键值对数据，没有特殊的结构约束，数据以 base64 编码形式存储
加密使用base64进行加密，就是转换一种编码方式,echo的-n选项表示不输出末尾的换行符
] echo -n "admin" | base64 
YWRtaW4=
解密
] echo -n "YWRtaW4=" | base64 -d

Secret内的数据被拿出来使用时，一定会自动解码，所以在Secret中的k=v里的v一定要写加密后的值
kubectl describe Secret不能获得加密的值，可以通过kubectl get Secret -o yaml获得加密后的值，再通过base64解密
```

### 卷挂载

![Secret-Opaque卷挂载](6-k8s%E5%AD%98%E5%82%A8.assets/Secret-Opaque%E5%8D%B7%E6%8C%82%E8%BD%BD.png)

```
这个defaultMode的值转换成8进制就是挂载的文件的权限
256的8进制400
420的8禁止644
```

# subPath挂载

## 一般使用mountPath挂载

![mountPath挂载](6-k8s%E5%AD%98%E5%82%A8.assets/mountPath%E6%8C%82%E8%BD%BD.png)

```
使用mountPath挂载，支持热更新。
挂载会覆盖原本目录下的所有文件(删除原文件)
```

## 使用subPath挂载

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-with-subpath
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        # 使用subPath挂载单个配置文件
        - name: nginx-config  # 引用下面定义的卷
          mountPath: /etc/nginx/conf.d/custom.conf  # 容器内的目标路径
          subPath: custom.conf  # 卷中要挂载的文件名（对应ConfigMap中的键）
        # 可选：挂载另一个文件到不同路径
        - name: nginx-config
          mountPath: /etc/nginx/extra-headers.conf
          subPath: headers.conf
      # 定义卷（引用ConfigMap）
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-custom-config  # 提前创建的ConfigMap名称
          items:
          - key: custom.conf  # ConfigMap中的键
            path: custom.conf  # 卷内的文件名（与subPath对应）
          - key: headers.conf
            path: headers.conf
```

```
上图使用了subPath挂载方式进行挂载卷，该挂载方式不会删除挂载目录下的其它文件，挂载的文件是一个副本而不是软链接，所以不支持热更新
```

# DownwardAPI

```
DownwardAPI是一个机制，用于将pod或容器自身的元数据（例如pod名称、命名空间、标签、注解等）注入到容器内部，让容器内的应用程序能够便捷地获取到这些信息


获取 Pod 元数据
允许容器内的应用获取当前 Pod 的基本信息，例如：
Pod 的名称、命名空间
Pod 的 IP 地址
容器的 CPU / 内存资源限制和请求
Pod 的标签（labels）和注解（annotations）
动态配置应用
应用程序可以根据这些元数据动态调整自身行为，而无需硬编码配置。例如：
根据 Pod 名称区分不同实例
根据命名空间实现环境隔离（如开发 / 测试 / 生产环境）
根据资源限制调整应用的内存使用策略
```

## 例子

![DownwardAPI添加env](6-k8s%E5%AD%98%E5%82%A8.assets/DownwardAPI%E6%B7%BB%E5%8A%A0env.png)

```
当给pod添加环境变量时，可以使用如上方式。通过将元数据等数据以环境变量地方式注入容器。上面是以环境变量地形式挂载，不支持热更新
```

## 例子2

![DownwardAPI卷挂载](6-k8s%E5%AD%98%E5%82%A8.assets/DownwardAPI%E5%8D%B7%E6%8C%82%E8%BD%BD.png)

![DownwardAPI卷挂载2](6-k8s%E5%AD%98%E5%82%A8.assets/DownwardAPI%E5%8D%B7%E6%8C%82%E8%BD%BD2.png)

```
以上是以DownwardAPI制作卷，卷挂载pod地方式，该方式生成地文件是链接文件，支持热更新
```

## DownwardAPI-拓展

```
DownwardAPI只能暴露pod自身信息给pod内部地容器，无法暴露其它pod地信息给pod内部地容器使用。
如果想要访问其它pod地信息，需要通过其它方式
```

### APIServer

```
创建RBAC，让pod有权限访问APIServer。
要有k8s集群管理员地token，放在请求头里
要指定证书，使用https访问，应为APIServer默认监听6443，只接受https请求
然后可以让pod里容器里地程序使用HTTPS访问APIServer

命令行方式访问
例如curl访问 curl 指定请求头中带管理员token进行身份验证 指定证书 访问路径
就可以使用curl命令访问APIServer获取其它pod地信息

关于访问路径：
首先在default空间下有kubernetes默认svc，提供给pod访问APIServer
访问路径https://kubernetes.default.svc.cluster.local/v1/namespaces/$NS/pods
这个$NS换成要需要地pod信息地pod所在命名空间。这个路径会获得对应命名空间下所有pod地信息
```

### 命令kubectl proxy

```
上述方式访问APIServer比较复杂，需要配置很多东西。在命令行还有简单方式访问
使用kubectl proxy --port=8080
这条命令是在本地临时创建一个代理服务器，将命令行地请求代理到APIServer上。
kubectl proxy会在本地启动一个服务(默认绑定127.0.0.1，端口通过--port=端口来指定)，该代理会：
· 自动使用~/.kube/config中地配置与APIServer通信
· 将本地地HTTP请求转发到APIServer，并返回响应

这样就简化了命令行访问APIServer地方式，无需手动处理证书或者Token


```



# Volume

```
pod内地容器如果崩溃，kubelet会重构它，但是容器内地文件就丢失了，为了容器之间共享文件，kubernetes就用Volume这个抽象解决问题
```

## Volume-emptyDir

```
当pod被分配给节点时，首先会自动在节点上创建emptyDir这个volume，卷初始状态为空
该emptyDir卷和pod的生命周期绑定，当pod从节点上被删除时，对应的emptyDir中的数据会被删除
当pod中的容器崩溃时，不会从节点上移除pod，emptyDir卷中的文件不会被删除。当pod内新容器被创建出来后会挂载这个emptyDir卷

emptyDir卷的路径在节点/var/lib/kubelet/pods/{podId}/volumes/kubernetes.io~empty-dir/
注：pod内的容器不会自动挂载这个卷，需要在配置文件中显示的挂载这个卷

pod的id可以在配置文件中查看uid ]kubectl get pod podname -o yaml

用途场景：
容器间共享数据：同一 Pod 内的多个容器可以挂载同一个 emptyDir 卷，实现数据共享。
临时缓存空间：例如供应用程序临时写入中间数据、日志缓存等。
临时存储：某些应用需要临时磁盘空间时使用（如解压文件、临时计算等）。
```

### 例子

![volume之emptyDir卷](6-k8s%E5%AD%98%E5%82%A8.assets/volume%E4%B9%8BemptyDir%E5%8D%B7.png)

```yaml
上图中的pod内两个容器挂载了emptyDir卷，一个容器的nginx启动后，可以在另一个容器中看到创建的文件access.log中有访问记录，因为都挂在同一个emptyDir卷

empty制作卷的方式：
1.基础方式（默认磁盘存储）
使用空对象{}定义，默认使用本节点的本地磁盘作为存储介质
volumes:
- name: my-empty-dir
  emptyDir: {} #使用默认的磁盘存储
  
2.指定存储介质为内存(tmpfs)
volumes:
- name: my-memory-dir
  emptyDir:
    medium: Memory	#使用内存作为介质
    sizeLimit: 128Mi	#限制使用最大内存128M，不能超过pod的限制内存
```

## Volume-hostPath

```
将节点上的本地文件或者目录挂载到pod中的卷类型。
hostPath生命周期和节点绑定，pod被删除，hostPath依然留在节点上。不同节点数据不共享


用途场景：
运行需要访问节点本地文件的应用（如监控节点的日志文件 /var/log）。
临时存储需要跨 Pod 生命周期保留的数据（但不适合生产环境的持久化需求）。
开发 / 测试场景中快速挂载本地文件到 Pod。

默认配置
volumes:
- name: host-path-demo
  hostPath:
    path: /data/hostpath  # 节点上的路径
    
指定类型（type）
通过 type 字段控制路径的行为，常用类型包括：
不指定/空：不做任何检查
DirectoryOrCreate：若路径不存在则创建为目录（默认）。
Directory：必须存在且为目录，否则 Pod 启动失败。
FileOrCreate：若路径不存在则创建为文件（需确保父目录存在）。
File：必须存在且为文件，否则 Pod 启动失败。
Socket/CharDevice/BlockDevice：分别要求路径是 socket 文件、字符设备、块设备。
volumes:
- name: node-logs
  hostPath:
    path: /var/log
    type: Directory  # 严格要求是已存在的目录
```

### 例子

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-demo
spec:
  containers:
  - name: demo-container
    image: busybox
    command: ["sh", "-c", "echo 'hello from hostPath' > /host/data.txt && sleep 3600"]
    volumeMounts:
    - name: host-volume
      mountPath: /host  # 容器内的挂载路径
  volumes:
  - name: host-volume
    hostPath:
      path: /data/hostpath-demo  # 节点上的路径
      type: DirectoryOrCreate  # 不存在则自动创建目录
```

​	

# PersistentVolume(PV)和PersistentVolumeClaim(PVC)

```
用于管理持久化存储和存储资源管理的问题。
PV(持久卷)：由集群管理员预先创建，是对底层存储资源(NFS、CEPH、EMC2)的抽象，定义存储类型、容量、访问模式
PVC(持久卷声明)：对存储资源的需求，声明需要的存储容量、访问模式。按照需求自动绑定最合适的PV

PV是具备实际存储能力的资源，PVC是用户对PV的申请单
```

## 关系图

![PV和PVC的关系](6-k8s%E5%AD%98%E5%82%A8.assets/PV%E5%92%8CPVC%E7%9A%84%E5%85%B3%E7%B3%BB.png)

## PV/PVC-关联条件

```
容量：PV的值要大于等于PVC的要求，最好一致
读写策略：PV 必须能够满足 PVC 所声明的全部访问模式
	1、单节点读写-ReadWriteOnce-RWO
	2、多节点读写-ReadOnlyMany-ROX
	3、多节点读写-ReadWriteMany-RWX
存储类：PV的类与PVC的类必须保持一致，不存在包容降级关系
```

### 读写策略(Access Modes)

```
RWO：单节点读写，只能被单节点以读写方式挂载
ROX：多节点读，只能被多节点以读方式挂载
RWX：多节点读写，只能被多节点以读写方式挂载

1. 读写策略只与节点挂钩，与pod无关
例如单节点读写RWO，可以被同一节点上多个pod挂载，单禁止跨界点挂载

2. 读写策略依赖底层存储驱动支持
并非所有的底层存储类型支持这三种访问方式，需要结合PV的底层存储方式进行判断
本地磁盘(Local)：仅支持RWO
网络文件系统(NFS)：3中都支持
云存储(AWS、EBS)：仅支持RWO
分布式存储(GlusterFS)：3中都支持
Ceph RBD：RWO、ROX

3. PVC和PV的读写策略匹配
一个PV可以声明一个访问策略，也可以声明两个访问策略，不考虑其它只要PV的访问策略满足PVC的要求，两者就能匹配

PVC的RWO和PV的RWO、ROX、RWX、RWO-ROX...都匹配
PVC的RWX和PV的RWX匹配
```

### 存储类(StorageClass)

```
PV和PVC的存储类必须一致，所以可以认为指定各种存储类，一级存储、二级存储等，这样业务写PVC就不需要惯性PV，让存储去实现PV
```

### 回收策略(Reclaim Policy)

```
Retain(保留)：PVC删除后，PV保留(状态变为Released)，底层数据不会被删除，需要手动清理。手动清理数据，重新				配置PV后，才能被PVC绑定。生产环境，数据很重要
Delete(删除)：PVC删除后，PV自动删除，同事PV的底层存储资源也被自动清理，数据自动删除。开发、测试环境
Recycle(回收、已废弃)：PVC删除后，PV自动清理，PV重新变成Available状态，可被PVC绑定。不推荐，已废弃
```

### 状态

```
PV有五种状态：
Available(可用状态)：空闲资源，未被绑定
Bound(已绑定)：卷已被绑定
Released(已释放)：PVC被删除，PV保留，未被重新声明，不可被绑定
Terminating(正在删除)：PV正在被删除
Failed(失败)：回收过程出错，异常状态
```

### PVC保护

```
核心保护机制：确保正在被pod使用的PVC不被意外删除。
PVC保护是为了解决--误删PVC导致业务故障：
1. PVC删除后，PV会按照回收策略被回收
2. 运行的pod会因为挂载的PVC消失，出现存储卷失效，pod无法读写数据，业务中断
3. 若PV采用delete策略，数据会被永久删除

PVC保护的核心逻辑：
1. 判断PVC是否正在使用中
存在至少一个运行中的 Pod（状态为Running或Pending，且Pending是因其他资源不足而非存储问题）；
该 Pod 的spec.volumes.persistentVolumeClaim.claimName字段明确引用了此 PVC（即 Pod 已挂载该 PVC）。
简单说：只要有 Pod 在 “用” 这个 PVC，它就是 “在使用中” 的。
2. 阻止在使用中的PVC被删除
当用户执行kubectl delete pvc <pvc-name>删除 “在使用中” 的 PVC 时，K8s 会：
拒绝立即删除 PVC，而是将 PVC 的metadata.finalizers字段添加kubernetes.io/pvc-protection标记（finalizers是 K8s 的 “删除拦截器”，存在该标记时，资源无法被真正删除）；
PVC 状态会显示为Terminating（正在删除），但实际会 “卡住”，直到其不再被任何运行中的 Pod 使用。
3. 允许 “未使用” 的 PVC 删除
当 PVC 不再被任何运行中的 Pod 使用时（如所有引用该 PVC 的 Pod 已被删除或停止），K8s 会：
自动移除 PVC 的kubernetes.io/pvc-protection最终器（finalizers）；
执行正常的删除流程，PVC 被彻底删除，其绑定的 PV 按回收回收策略处理（如Delete删除底层存储，Retain让 PV 进入Released状态）。
```



#### 例子

部署nfs服务器

```bash
] yum install -y rpcbind nfs-utils ; mkdir /data ; chown 666 -R /data ; for num in $(seq 1 10);do mkdir /data/${num} done ; for i in $(seq 1 10); do echo ${i} > /data/${i}/index.html

] for i in $(seq 1 10); do echo "/data/${i} *(rw,no_root_squash,no_all_squash,sync)" >> /etc/exports

] systemctl restart nfs-server
```

制作PV

![PV的配置文件](6-k8s%E5%AD%98%E5%82%A8.assets/PV%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.png)

```
server就是nfs服务器的地址，path要改成共享的目录之一，比如/data/1。
每一个共享目录都可以制作一个PV
```

创建无头模式服务

![Headless Service](6-k8s%E5%AD%98%E5%82%A8.assets/Headless%20Service.png)

```bash
这里的VIP是None，这样ipvs就无法为这个svc创建集群，这个svc就无法负载均衡
```

创建StatefulSet控制器和PVC

![StatefulSet控制器](6-k8s%E5%AD%98%E5%82%A8.assets/StatefulSet%E6%8E%A7%E5%88%B6%E5%99%A8.png)

![PVC的模板](6-k8s%E5%AD%98%E5%82%A8.assets/PVC%E7%9A%84%E6%A8%A1%E6%9D%BF.png)

##### 关于无头服务

```bash
kubectl get svc
#可以看到之前创建的svc是没有ClusterIP的，那么IPVS就无法为这个svc以及svc关联的pod创建集群

kubectl get endpoint
#可以看到这个无头服务的ep是有关联的3个pod的ip:端口，即端点的。

#虽然不能创建集群作负载均衡，但是可以作DNS解析记录。例如：无头服务叫nginx
dig -t A nginx.default.svc.cluster.local @10.0.0.10
#会发现，该无头服务的DNS解析结果以A记录的形式解析到关联的pod的DNS上。不是负载均衡，是以A记录的形式解析到pod上
```



### PV释放

```bash
和PVC绑定的PV处于Bound状态，当对应的PVC被删除后，PV会变成Released状态。
此时PV不可被绑定。
方式一：
删除PV，重新创建

方式二：
将PV中的claimRef字段全部删除，这个字段是记录什么PVC绑定这个PV。这个字段删除后，PV将变回Released状态
```

# StorageClass

一种动态申请存储的机制

例子

![image-20250903231041513](6-k8s%E5%AD%98%E5%82%A8.assets/image-20250903231041513.png)

```
创建storageClass，然后通过RBAC给这个storageCLass授权。这样pod绑定PVC，PVC寻找符合条件的PV，如果没有就让这个StorageClass去创建符合条件的PV，然后PVC和这个PV绑定
```

