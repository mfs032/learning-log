# k8s组件

组件缺一不可，少掉一个都不完整



## 宏观

宏观分为Master节点和Node节点，Master节点用来控制Node节点，Node节点实际运行服务



## 微观

### Master节点组件

#### kubectl

kubectl：k8s命令行客户端



#### Web UI

web UI(官方默认不部署)：Web 界面客户端，社区有DashBoadr仪表盘作UI，也可以安装第三方，比如Rancher



#### API Server

API Server：遵循Restful风格被开发，接受kubectl,web UI ，scheduler，replication controller发来的请求，审核请求通过后，将数据存储到etcd中

##### Restful风格

访问的地址不变，通过不同的请求方式实现不同的功能。这是一种编程方式，降低编程的复杂度

http://127.0.0.1/user/1	GET方式根据用户id查询数据

http://127.0.0.1/user/1	POST方式新增用户

http://127.0.0.1/user/1	PUT修改用户

http://127.0.0.1/user/1	DELETE删除用户



#### etcd

etcd：键值对数据库，key-value存储方式，即整个集群的信息会被保存到etcd中。Master节点要有3个就是为了etcd能有3个节点，这3个节点可以组成etcd class的集群。任何一个节点的损坏不会导致信息的丢失



#### scheduler

scheduler：调度器。调度器会绑定我们需要运行的容器和节点之间的关系，对应的节点就会运行容器，负责在合适的节点上部署Pod



#### Replication Controller

replication controller(controller manager)：集群管家，当一个Pod挂了，replication controller会保证有一个新的Pod去顶替，保障当前集群可用。



以上节点组成一个Master节点

#### 流程

流程：通过kubectl发送请求给API Server，API Server审核(权限等)，没问题会将任务保存到etcd中，scheduler定期通过API Server访问etcd获取需要运行的任务有没有和节点绑定，如果有没绑定的任务，scheduler会找最合适的一个节点和这个任务绑定。replication controller定期检查集群运行的状态是否和预期状态相符。

![Master节点组件](k8s%E7%BB%84%E4%BB%B6.assets/Master%E8%8A%82%E7%82%B9%E7%BB%84%E4%BB%B6.png)

### Node节点组件

#### kubelet

kubelet:监听API Server，如果收到请求，会调用CRI(容器运行时，docker、podman，containerd)去创建容器，主要负责容器的创建和删除



#### kubeproxy

kubeproxy：监听API Server，发现当前节点上有没有防火墙规则，负载均衡规则需要去创建，主要负责节点网络和负载均衡的问题



#### Pod

Pod：k8s最小部署单位，k8s里不部署容器而部署Pod，将多个容器组成一个逻辑分组为一个Pod。

# Pod概念

## Pause容器

### 为什么要有Pause容器

Pod内部第一个启动的容器、初始化网络栈、挂在需要的存储卷、回收僵尸进程。由于Pause启动后几乎处于休眠不容易挂，由它共享给其它容器网络等更稳定

同一个Pod里会有多个容器，例如Pause、Nginx、redis、php-fpm，这些同一个Pod里的容器共享命名空间(Network，PID，IPC)

Network：容器共享网络，是为了方便容器之间通信

PID、IPC：这两个共享是为了让Pause容器里的进程成为上帝进程，这样才能回收其它容器产生的僵尸进程



比如Pause、nginx、php-fpm组成一个逻辑Pod，Pause、nginx、php-fpm共享network，ipc，pid，Pause的80端口映射到物理机的8080端口，nginx监听80端口，php-fpm监听7900端口，

当请求发送给物理机的8080端口，被转发到Pause的80端口，由于nginx和Pause共享network,ipc,pid，所以nginx页监听到这个请求，给这个请求作转发127.0.0.1:7900,发给php-fpm处理



# k8s网络

## 基本概述

k8s的网络模型假定了所有的Pod都是在一个可以直接联通的扁平的网络空间里，在搭建k8s集群时，要先将不同节点上的Docker容器之间的互相访问先打通，然后运行k8s



### 网络地址转换（NAT，Network Address Translation）

为解决ipv4地址短缺，同时增强网络安全，在路由器或者防火墙上，将内部网络的私有ip和公有网络的公有ip进行转换。



### k8s网络模型原则

1、在不使用网络地址转换(NAT)的情况下，集群中的Pod能与其它任意的Pod进行通信

2、在不使用网络地址转换(NAT)的情况下，在集群节点上运行的程序，能与同一节点上的任何Pod进行通信

3、每个Pod都有自己的IP地址(独立唯一)，并且任意其它Pod都可以通过相同的这个地址访问它



### 

## CNI（Container Network Interface）

CNI是一种定义容器网络配置和管理的标准化接口规范，用户可以通过不同的网络插件来满足特定的需求，为了实现k8s节点默认的CNI插件路径为/opt/cni/bin

基于CNI开发出来的插件 的核心作用是解决 Kubernetes 集群中容器的网络连接问题，包括：

- 为容器分配 IP 地址
- 实现容器之间的网络通信
- 实现容器与外部网络的通信
- 处理容器创建 / 销毁时的网络配置变更



CNI的接口是指通过对应的命令，调用对应的可执行程序



## CNI插件功能

1、kubelet在通过CRI调用容器运行时创建容器的过程中，会调用CNI插件

2、CNI插件给Pod中的每个容器配置网络接口

3、分配IP地址并配置路由规则

4、确保Pod能与集群内其它Pod及外部网络通信

5、当Pod被删除后，CNI插件负责清理相关网络资源



### CNI插件——Calico

#### Felix

管理网络接口、编写路由、编写ACL、报告状态

#### bird

BIRD（BGP Routing Daemon）路由守护进程，主要负责在 Calico 集群中分发路由信息，实现容器之间的跨节点通信。

1. **路由信息交换**
   每个节点上的 BIRD 进程会将本节点上容器的 IP 地址（Pod IP）和所在节点的物理 IP 之间的对应关系，通过 BGP 协议告知集群中的其他节点。
   例如：节点 A 上有容器 IP 为 `10.244.1.2`，BIRD 会将 “`10.244.1.2/32` 对应的下一跳是节点 A 的物理 IP” 这个路由规则同步给其他节点，让其他节点知道如何访问这个容器。
2. **维护路由表**
   BIRD 会在每个节点上维护一份动态更新的路由表，记录整个集群中所有容器 IP 的可达路径。当容器创建 / 删除或节点上线 / 下线时，BIRD 会自动更新路由表并同步给其他节点，确保路由信息实时准确。
3. **支持 BGP 路由反射器（Route Reflector）**
   在大规模集群中（如超过 100 个节点），为了避免节点间建立全量 BGP 连接（会导致网络开销过大），Calico 会部署 BIRD 作为 “路由反射器”。此时，其他节点只需与路由反射器交换路由信息，而非两两互联，大幅降低了网络复杂度



- BIRD 本身是一个独立的开源软件（并非 Calico 专属），被 Calico 集成作为 BGP 协议的实现工具。
- Calico 的网络模式分为BGP 模式（非封装）和IPIP 模式（封装）：
  - 在 BGP 模式下，BIRD 是核心组件，直接通过 BGP 分发路由，实现高性能的跨节点通信。
  - 在 IPIP 模式下，虽然数据包会被封装，但 BIRD 仍会参与路由信息的交换（只是最终转发时会添加 IPIP 封装头部）。
- Calico 会为 BIRD 生成自动配置文件，确保其按 Calico 的网络规则运行，用户通常无需手动修改 BIRD 配置。



#### confd



### VXLAN

**VXLAN 的核心特点**

1. **突破 VLAN 限制**
   传统 VLAN 使用 12 位标签，最多支持 4096 个网络隔离。而 VXLAN 使用 24 位的**VXLAN 网络标识（VNI）**，可支持约 1600 万个虚拟网络，满足大型数据中心或云环境中多租户隔离的需求。

2. **封装机制**
   VXLAN 会将原始的二层以太网帧（如虚拟机 / 容器发送的数据包）封装到**UDP 数据包**中，并添加 VXLAN 头部（包含 VNI），然后通过三层 IP 网络传输。这种 “隧道” 机制使虚拟网络可以跨物理子网部署。

   封装结构大致为：
   `[外层IP头部] + [外层UDP头部] + [VXLAN头部（含VNI）] + [原始以太网帧]`

3. **跨物理网络通信**
   虚拟网络中的设备（如虚拟机、容器）只需知道同一 VNI 下的 MAC 地址构建原始以太网帧，无需关心底层物理网络的拓扑。将数据发送给本地的 VTEP（VXLAN 隧道端点）。VTEP 会根据原始以太网帧中的目标地址等信息，结合自身维护的映射表，确定目标 VTEP 的 IP 地址，并将原始以太网帧封装在 UDP 报文中，添加外层 IP 头部（目标 IP 为目标 VTEP 的 IP 地址）后，通过底层物理网络发送到目标 VTEP。目标 VTEP 解封装后，再根据原始以太网帧中的目标 MAC 地址，将数据转发给目标虚拟机或容器。

4. **VXLAN 的封装和解封装主要在内核态进行**

   这是由 Linux 内核原生支持的标准方式，能够在保证隔离性的同时提供较高的转发性能

### **VXLAN 的关键组件**

- VTEP（VXLAN Tunnel Endpoint，VXLAN 隧道端点）：

  负责 VXLAN 数据包的封装与解封装，通常部署在物理机、路由器或交换机上。例如：

  - 虚拟化环境中，宿主机可作为 VTEP，为其内部的虚拟机处理封装。
  - 容器集群中，每个节点可作为 VTEP，为 Pod 的数据包提供封装。



### 基于 Linux 虚拟网络设备实现的网络通信原理

- **Veth Pair 虚拟设备**：Linux 中的 Veth Pair 总是以两张虚拟网卡的形式成对出现，从其中一个网卡发出的数据包，可以直接出现在与它对应的另一张网卡上，哪怕这两张网卡在不同的 network namespace 中。在容器网络中，Veth Pair 一端连接在 Pod 上，作为 Pod 的 eth0，另外一端连接在物理机上。
- **网桥（Bridge）**：bridge 是 Linux 内核的虚拟交换机，连接了节点上所有容器的虚拟网卡（Veth Pair 的一端）。当数据包到达 bridge 后，bridge 会根据目标 MAC 地址，将数据包转发到对应的容器 Veth Pair 另一端，最终进入 Pod 的网络命名空间，到达 Pod 的网卡（`eth0`）。
- **VTEP**：由 Linux 内核模块实现，表现为节点上的一个虚拟网络接口，Calico 使用 VXLAN 时创建的`vxlan.calico`接口，当涉及跨节点的 Pod 通信时，就需要 VTEP 发挥作用。VTEP 是 VXLAN 隧道端点，通常每个宿主机上都会创建一个 VTEP 设备。它的主要作用是对数据包进行 VXLAN 封装和解封装。当一个 Pod 要向其他节点的 Pod 发送数据时，数据包从本节点网桥转发到 VTEP，VTEP 会对其进行封装，添加 VXLAN 头部、UDP 头部和外层 IP 头部等，然后通过物理网络发送到目标节点的 VTEP，目标节点的 VTEP 解封装后，再将原始数据包通过网桥转发到目标 Pod。
- **数据包转发**：网桥会根据数据包的目标 MAC 地址，在其维护的 CAM 表（端口和 MAC 地址的对应表）里查找对应的端口，若目标端口是连接 Pod 的 Veth Pair 设备，则将数据包发往该端口，这样数据包就进入到了 Pod 的 Network Namespace 里，最终到达 Pod 的网卡。









# k8s插件

外部组件，扩展功能

## CRI

CRI容器运行时是实际负责创建容器的，例如docker、podman、containerd



## CoreDNS

给k8s内部提供一个私有的域名解析服务，这样节点在k8s集群内部就有了第二种访问方式，处理IP还可以通过域名访问



## Ingress Controller

提供7层的负载均衡功能



# k8s附件

和k8s集群没有关系，可以附带使用，起到更好的效果

## Prometheus

监控集群



## DashBoard

仪表盘组件



## Fedration

提供多k8s集群，跨空间，跨多个集群的管理能力