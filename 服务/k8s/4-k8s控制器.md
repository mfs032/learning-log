# Pod控制器分类

## 守护进程类型：

ReplicationSet、Deployment、DaemonSet

## 批处理任务类型：

Job

# Pod控制器--ReplicationSet

在k8s集群中运行了一系列控制器来确保集群的当前状态与期望状态保持一致。例如，replicaSet控制器负责维护集群中运行的Pod数量，Node控制器监控节点的状态



## RC(ReplicationController)和RS(ReplicationSet)

部署方式和Pod相同。首先RC只能管理由自己创建的Pod再匹配标签，如果不是自己创建的就不会匹配标签



删除控制器创建的Pod没有意义，因为控制器还会重新创建，要删除删除控制器本身，删控制器会自动删除控制器控制的Pod



在新版本使用RS替代RC，两者没有本质不同，只是名字不一样，并且RS支持集合式selector

RS是用来确保Pod的的数量始终保持在用户定义的数量，如果有Pod的容器异常退出，RS会自动创建新的Pod来替代。旧Pod里的容器会被回收

### 例子RC

![ReplicationController](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/ReplicationController-1755186179417-5.png)

```shell
spec字段下有一个selector字段，即选择器，对应的标签app: rc-demo，说明这个RC管理标签里带有app: rc-demo的Pod。
spec下的templete表示这个RC需要创建的Pod的信息，这个Pod的标签labels必须大于等于RC的选择器selector的标签，即选择器的标签是Pod标签的子集

] kubectl create -f xxx.yaml
#创建RC后，RC会根据配置文件里的模板创建Pod

] kubectl get pod -o wide
] kubectl get pod --show-labels

] kubectl label pod rc-demo-ssss ver=v1
#给pod rc-demo-ssss新加一个标签
#此时pod数量不变，因为RC选择器的标签是rc-demo-ssss的标签的子集

] kubectl label pod rc-demo-ssss app=newapp --overwrite
#更改原来的pod的标签
#此时再get pod发现会多一个pod，因为RC选择器的标签不再是rc-demo-ssss的标签的子集，此时RC会根据预期再创建一个新的Pod


] kubectl label pod rc-demo-ssss app=rc-demo --overwrite
#此时该pod又归RC管理，RC会杀死最近创建出来的pod，因为老的pod里面的数据更多

] kubectl scale rc rc-demo --replicas=10

```

### 例子RS

![ReplicationSet](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/ReplicationSet-1755186173111-3.png)

```shell
RS和RC没有本质上的区别，功能相同，RS多了一个集合式selector的功能

] kubectl explain rs.spec.selector
#匹配标签还是子集运算，和RC没区别
#多了匹配运算符
selector.matchExpressions
支持的操作包括：
In：label值在某个列表中
NotIN：label值不在某个列表中
Exists：某个label存在
DoesNotExists：某个label不存在
```

#### 例子RS--匹配运算符Exists

存在对应的key: app

![ReplicationSet-匹配运算符](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/ReplicationSet-%E5%8C%B9%E9%85%8D%E8%BF%90%E7%AE%97%E7%AC%A6.png)

#### 例子RS-匹配运算符In

键key的值是spring-k8s和hahahah之一

![ReplicationSet-匹配运算符2](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/ReplicationSet-%E5%8C%B9%E9%85%8D%E8%BF%90%E7%AE%97%E7%AC%A62.png)

## replace和apply的区别

```shell
replace会完全替换所有字段，无论字段是否相同
apply会更新不同字段

可以通过diff来比较区别

] kubectl diff -f xxx.yaml
#比较这个配置文件和当前配置文件的区别
```



# Pod控制器--Deployment(deploy)

通过控制RS来进行对Pod的编排调度，扩缩容，滚动更新，回滚。

deploy通过控制不同RS的期望值来控制pod在不同版本之间迭代

## 例子

![deployment](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/deployment.png)

### 常用命令

```
spec.replicas没有指定，默认1

] kubectl get deploy myapp-deploy -o yaml

] kubectl scale deploy myapp-deploy --replicas=10
```

## deoloyment和RS的关联

```shell
首先创建deployment对象(默认replicas=1)
->
由deployment创建ReplicationSet(默认replicas=1)
->
再由ReplicationSet创建Pod(创建deploy中template指定的Pod的replicas数量，默认1)
```

### deployment通过RS间接管理Pod

```
功能解耦：
	Pod：最小部署单元
	ReplicationSet：维持指定数量的Pod副本
	Deployment：通过控制RS实现版本升级、回滚等高等编排逻辑
```

#### 滚动更新

```yaml
更改deploy配置文件中的Pod的版本

deploy对象会创建一个新的RS对象
->
由新RS对象创建一个Pod对象
->
旧RS对象杀死一个Pod对象，这样完成将所有Pod由旧到新的更新

这样可以做到每时每刻都有Pod给用户访问
```

#### 回滚

```
] kubectl rollout undo deploy deployment-demo
```



##### 常用命令

```shell
] kubectl create -f deploy.yaml --record
#create创建资源，若存在会报错
#--record参数可以记录命令，我们可以很方便地查看每次revision的变化
#通过kubectl rollout history deployment/depmloyment-demo查看历史更新记录
] kubectl apply -f deploy.yaml
#apply若资源不存在会创建，若存在会比较，并修改目标资源配置文件的字段(部分更新)
] kubectl replace -f deploy.yaml
#完全替换，重建此对象
#比如replicas副本数量，如果没有明确指定副本数量apply会不变，replace会变成默认值1

] kubectl scale deploy nginx-deploy --replicas=10
#扩缩容

] kubectl set image deployment/deploy-name  deploy-name-container=pod-container-name
#deploy-name是deploy的名称，deploy-name-container是对应容器的名称，=右边指定容器名称


```

## deploy更新策略

```
deployment可以保证在升级时只有一定数量的Pod是down的。默认的，它会确认至少有比期望的Pod数量少一个是up状态(默认最多一个不可用)

deployment可以保证只创建出超过期望数量的一定数量的Pod。默认的，它会确保最多比期望的Pod数量多一个的Pod是up的(默认最多一个surge)

现在新版默认从1-1改为25%-25%
```

### 滚动更新

![deploy更新策略](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/deploy%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5.png)

```shell
] kubectl explain deploy.spec.strategy.type
#结果有两个
#Recreate
#rollingUpdate
#	maxSurge:指定超出副本数的Pod最多有几个，两种方式：1、指定数量	2、百分比
#	maxUnavailable:即允许多少旧 Pod 提前被删除
```

#### 金丝雀部署

```shell
核心思想：
在实际运行环境中的一小部分用户或流量上测试新版本软件，而大部分用户或流量任然使用旧版本。通过对新版本进行有限范围的实时测试和监控，可以及早发现潜在的问题，并减少对整个系统的影响

确认新版本没有问题，全部升级，有问题就回滚


可以通过apply的方式更新deploy，也可以通过patch打补丁的方式更新deploy,补丁如下
'{"spec":{"strategy":{"rollingUpdate":{"maxSurge":"1","maxUnavailable":"0"}}}'

对对象做小部分修改patch很合适
] kubectl patch deploy deployment-demo -p '{"spec":{"strategy":{"rollingUpdate":{"maxSurge":"1","maxUnavailable":"0"}}}'

] kubectl get deploy deployment-demo -o yaml	#查看补丁是否打上

#更改镜像版本的同时停止滚动更新
] kubectl patch deploy deployment-demo --patch '....' && kubectl rollout pause deploy deployment-demo

#恢复更新,将所有pod更新
] kubectl rollout resume deploy deployment-demo

#回滚,deploy通过控制不同RS的期望值来控制pod在不同版本之间迭代
] kubectl rollout undo deploy deployment-demo


#注意：为了方便查看版本迭代，最好在每一条更新命令之后加上--record参数，加上这个参数将会将更新命令记录在滚动历史中，如果没加则滚动历史会复制上一次滚动更新的命令
#例如通过更改镜像进行更新，加上--record
] kubectl set image deploy/deployment-demo deployment-demo-container=xxxx --record

#查看滚动更新历史
] kubectl rollout history deploy deployment-demo

#指定回滚的版本
] kubectl rollout undo deploy/deploy-demo --to-revision=2
```

![金丝雀部署](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/%E9%87%91%E4%B8%9D%E9%9B%80%E9%83%A8%E7%BD%B2.png)

### 手动滚动更新和回滚

```shell
可以将deployment不同版本的配置文件保留，当需要回到哪个版本就apply哪个版本的配置文件，这样实现手动滚动更新和回滚
```

## 多个rollout并行

```
当deploy从v1更新到v2，更新到一半时。此时立刻运行deploy更新到v3。
此时v1的版本会直接更新到v3,v2直接更新到v3。
```

## 简单创建deployment的配置文件

```shell
] kubectl create deploy deploy-demo --image=xxxx --dry-run -o yaml > simple-deploy-demo.yaml

--dry-run不实际创建pod
```







# Pod控制器--DaemonSet

DaemonSet会确保全部(或一些)Node节点上运行一个Pod的副本。当有Node加入集群时，DaemonSet会给他们新增一个Pod。当有Node从集群移除时，这些Pod会被回收。删除DaemonSet会删除它所创建的所有Pod。



Master节点默认有一个污点导致Pod不会被调度到Master节点，所以会有一些

## 例子

![DaemonSet-yaml](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/DaemonSet-yaml.png)



# Pod控制器-Job(专门用于批处理)

```
Job负责批处理任务，即仅执行一次的任务，他保证批处理任务的一个或多个Pod成功结束(返回状态码0)

Job的restartPolicy仅支持Never或OnFailure

spec.completions标志Job结束需要成功运行的Pod个数(失败不算)，默认1
spec.parallelism标志并行运行的Pod个数(最多同时存在的Pod数)，默认1
spec.activeDeadlineSeconds标志失败Pod的重试最大时间，超过这个时间不会重试
```

## 例子

![Job-yaml](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/Job-yaml.png)



# Pod控制器--CronJob

基于时间管理的Job：

	1. 在给定的时间点运行一次
	1. 周期的在给定的时间定运行

```yaml
spec.schedule：调度，必须字段，指定任务运行周期，格式分时日月周*****
spec.jobTemplete：Job模板，必须字段，指定需要运行的任务，格式同Job
spec.startingDeadlineSeconds：启动Job的期限，该字段可选，因任何原因而错过了被调度的时间，那么错过调度时间的Job这一次被认为是失败的并且会等到下一次周期时间再尝试创建。如果没有指定则没有期限

spec.concurrencyPolicy：并发策略，该字段可选，表示被CronJob创建的Job要不要并发运行，有如下策略：
	1. Allow(默认):允许并发运行Job
	2. Forbid：禁止并发运行，如果前一个未完成，则直接跳过下一个
	3. Replace：取消当前正在运行的Job，用一个新的来替换
	该策略只适用于同一个CronJob创建的Job，多个CronJob创建的Job总是允许并发执行的
	
spec.suspend：挂起，该字段可选。如果置为true，后续所有执行都会被挂起，对已经开始执行的Job不起作用，默认值false
spec.successfulJobsHistoryLimit和spec.failedJobsHistoryLimit：默认值3和1。指保留的多少完成的和失败的Job
```

## 例子

![CronJob-yaml](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/CronJob-yaml.png)



# Pod控制器--StatefulSet

## 有状态服务(Stateful Service)和无状态服务(Stateless Service)

```
状态：服务在运行过程中需要持久化的一些信息

无状态服务：不需要保存状态，例如静态前端界面，

有状态服务：需要保存状态，后续请求的处理依赖历史保留下状态，例如数据库、缓存服务(Redis集群)、消息队列(Kafka、RabbitMQ)、分布式存储
```

## StatefulSet展示

部署nfs服务器

```bash
] yum install -y rpcbind nfs-utils ; mkdir /data ; chown 666 -R /data ; for num in $(seq 1 10);do mkdir /data/${num} done ; for i in $(seq 1 10); do echo ${i} > /data/${i}/index.html

] for i in $(seq 1 10); do echo "/data/${i} *(rw,no_root_squash,no_all_squash,sync)" >> /etc/exports

] systemctl restart nfs-server
```

制作PV

![PV的配置文件](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/PV%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.png)

```
server就是nfs服务器的地址，path要改成共享的目录之一，比如/data/1。
每一个共享目录都可以制作一个PV

accessModes: [ "ReadWwriteOnce" ]
```

创建无头模式服务

![Headless Service](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/Headless%20Service.png)

```bash
这里的VIP是None，这样ipvs就无法为这个svc创建集群，这个svc就无法负载均衡
```

创建StatefulSet控制器和PVC

![StatefulSet控制器](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/StatefulSet%E6%8E%A7%E5%88%B6%E5%99%A8.png)

![PVC的模板](4-k8s%E6%8E%A7%E5%88%B6%E5%99%A8.assets/PVC%E7%9A%84%E6%A8%A1%E6%9D%BF.png)

##### 关于无头服务

```bash
kubectl get svc
#可以看到之前创建的svc是没有ClusterIP的，那么IPVS就无法为这个svc以及svc关联的pod创建集群

kubectl get endpoint
#可以看到这个无头服务的ep是有关联的3个pod的ip:端口，即端点的。

#虽然不能创建集群作负载均衡，但是可以作DNS解析记录。例如：无头服务叫nginx
dig -t A nginx.default.svc.cluster.local @10.0.0.10
#会发现，该无头服务的DNS解析结果以A记录的形式解析到关联的pod的DNS上。不是负载均衡，是以A记录的形式解析到pod上
```

### StatefulSet特性

#### 有序创建删除

```
有序创建：
StatefulSet创建的pod实例是有序创建，不想deploy一次将全部都创建出来。
即上一个pod内有running，下一个pod不允许创建
有序回收：
模板数改为0，按照创建的倒序依次回收，
```

#### 数据持久化

```
pod级别数据持久化
pod > pvc > pv >nfs
```

#### 稳定的网络访问模式

```
Headless Service 解决了这个问题：

它为每个 StatefulSet Pod 生成一条稳定的 DNS SRV 记录，格式为：

<pod-name>.<headless-service-name>.<namespace>.svc.cluster.local
示例：一个名为 mysql的 StatefulSet 和一个名为 mysql-read的 Headless Service。
Pod mysql-0的稳定地址是：mysql-0.mysql-read.default.svc.cluster.local
Pod mysql-1的稳定地址是：mysql-1.mysql-read.default.svc.cluster.local


对应的nginx的默认域名A记录到关联的pod，只需要访问这个nginx的默认域名
访问关联的pod
curl http://nginx.default.svc.cluster.cluster.local
访问特定的pod
curl http://www-web-0.nginx.default.svc.cluster.local
```

