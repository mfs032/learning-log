# 定时任务执行kubectl缺少依赖

## 现象

```
在主机54.219.33.54上添加定时任务，定时任务使用了kubectl get pod -A -o yaml命令，返回结果如下
{
    "apiVersion": "v1",
    "items": [],
    "kind": "List",
    "metadata": {
        "resourceVersion": "",
        "selfLink": ""
    }
}

```

## 原因

```
这个空列表的输出，实际上是 kubectl 客户端在认证失败后返回的结果，而不是 Kubernetes 服务端返回的。当 kubectl 无法完成认证时，它并不会直接返回一个明确的认证失败错误（比如 Error: Unauthorized）。相反，它会返回一个空的 API 对象。这是 kubectl 客户端的一个设计选择

返回一个有效的 API 对象：{} 是一个有效的 JSON 对象，代表一个空的列表。
避免直接的错误信息：这样设计可能是为了在某些情况下隐藏底层错误细节，防止信息泄露，或者只是为了保持输出的一致性。
```

```bash
在k8s组件之间使用的是双向TLS认证，kubectl与API Server进行通信的认证如下：

客户端认证服务端：
kubectl 向 Kubernetes API 服务器发起请求时，服务器会提供自己的证书。kubectl 使用其内置的或 kubeconfig 中配置的 CA (Certificate Authority) 证书来验证 API 服务器证书的真实性。这一步是为了确保 kubectl 连接到了正确的集群，而不是一个伪造的服务器。

服务端认证客户端：
kubectl 接收到 API 服务器的证书后，也会向服务器出示自己的客户端证书。API 服务器会使用它的 CA 证书来验证 kubectl 的客户端证书。如果验证通过，服务器就知道这个请求是来自一个合法的客户端，并给予相应的权限。


而在 AWS EKS(Elastic Kubernetes Service) 的环境中，kubectl 并不直接持有自己的证书，而是使用 aws 命令来获取一个短暂有效的身份令牌。这个令牌在双向认证的流程中，就扮演了客户端证书的角色。它证明了 kubectl 是一个合法的用户，可以与 API 服务器进行安全通信。

aws示例如下
[root@ip-172-31-1-148 ~]# ll /usr/local/sbin
total 248080
lrwxrwxrwx. 1 root root       18 Sep 14  2021 aws -> /usr/local/bin/aws
-rwxr-xr-x. 1 root root 76775424 Sep  8  2021 eksctl
-rwxr-xr-x  1 root root 45072384 Apr 11  2022 helm
-rwxr-xr-x  1 root root 85782528 Apr 11  2022 istioctl
-rwxr-xr-x. 1 root root 46403584 Sep 13  2021 kubectl
[root@ip-172-31-1-148 ~]# ll /usr/local/bin/aws
lrwxrwxrwx. 1 root root 37 Sep 14  2021 /usr/local/bin/aws -> /usr/local/aws-cli/v2/current/bin/aws
[root@ip-172-31-1-148 ~]# ll /usr/local/aws-cli/v2/current/bin/aws
lrwxrwxrwx. 1 root root 11 Sep 14  2021 /usr/local/aws-cli/v2/current/bin/aws -> ../dist/aws
[root@ip-172-31-1-148 ~]# ll /usr/local/aws-cli/v2/current/dist/aws
-rwxr-xr-x. 1 root root 4477296 Sep 14  2021 /usr/local/aws-cli/v2/current/dist/aws
```

```
而定时任务默认的$PATH=/usr/bin:/bin
默认路径不包括/usr/local/sbin。在/usr/bin和/bin下有kubectl命令，但是没有aws命令，导致kubectl无法获得认证自己身份的凭证
```

## 方法

```bash
在脚本执行前声明变量
PATH=$PATH:/usr/local/sbin
```



# 定时任务设置时间

## 现象 原因

```
如果定时任务统一设置为5的倍数，那么在某一时刻，cpu负荷会比较高
```







# nginx故障

## 问题诊断

```bash
图片服务器nginx代理缓存被快速存满

缓存系统存在恶行循环，导致热点Key持续MISS，总命中率极低
1.业务导致的Key分裂(关键原因):Nginx默认配置使用的proxy_cache_key "$scheme$host$request_uri"业务系统将客户分组，不同组的客户请求不同域名，使得Nginx为同一张图片创建多个缓存副本，导致流量分散，缓存资源被浪费

2.操作系统导致的LRU失效：使用tmpfs作为Nginx的混村，但tmpfs的挂载点使用率noatime,nodiratime选项，使得Linux阻止更新缓存文件的访问时间，导致Nginx的LRU机制无法判断Key的实际热度，在tmpfs达到规定的max_size后会错误地删除热点Key

3.写入策略不合理：最初地写入策略无法抵御海量地长尾流量污染
```

## 修复步骤

```bash
1.修复Key分裂
配置项proxy_cache_key
原配置 "$scheme$host$request_uri"
	$scheme指使用的协议列入HTTP、HTTPS
	$host指请求地主机名或者IP地址
	$request_uri指请求地完整路径，包括?后面地查询参数
优化后地配置 "$scheme$uri"
	$uri指请求地路径，不包括?后地参数

更改完Key结构后，必须清空旧缓存，否则旧缓存无法被命中，且无法被LRU清理，将锁死在内存空间
rm -rf /path/to/proxy_cache_path/
或者
find /path/to/proxy_cache_path -type f | rm -rf
```

```bash
2.修复LRU机制
配置项/etc/fstab中tmpfs文件系统地选项
原配置noatime,nodiratime
优化后地配置：移除这两个选项，使用默认地defaults

作用：恢复了Nginx对tmpfs文件系统中文件和目录地atime地记录，是的LRU算法可以准确判断Key地热度

更改完fstab可以使用mount重新挂载tmpfs系统，不需要重启主机
```

```bash
3.优化写入和保护策略
配置项proxy_cache_min_uses
默认1
优化后2
作用阻止了大多数一次性(长尾)请求污染缓存，同时允许热点Key在第二次请求时即可写入，平衡缓存质量和写入速度

配置项proxy_cache_valid
初始5m
优化后12h
作用：静态图片更改非常少，缓存设置长一些，减少因为Key过期导致地MISS

配置项proxy_cache_revalidate
默认off
优化后on
作用：启用304验证，这样Key过期不需要向后端发送完整地新请求，只需发送条件请求，当后端服务器返回304时，Nginx会服用就缓存并延长其有效期。若后端返回新内容，则更形缓存
```

## 最终结果

```
热点Key不再被LRU去除，并且稳定命中
整体命中率从7%~8%上升到60%左右
```



# 集群可用pod监控失效

## 问题诊断

```bash
主机负责联络两台集群，脚本中未加上切换集群的命令，导致无法准确监控目标集群
```

## 修复

```bash
脚本加上切换集群的命令
aws eks update-kubeconfig  --region us-east-1  --name eks-online-cluster-01
```

# 最终结果

```
告警生效
```

